\documentclass[12pt,fleqn,twocolumn]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}

\usepackage[top=2.5cm, bottom=2.5cm, left=3cm, right=3cm, includeheadfoot]{geometry}

\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{float}
\usepackage{changepage}
\usepackage[nottoc, numbib]{tocbibind}
\usepackage{lastpage}
\usepackage{setspace}
\usepackage[bottom]{footmisc}
\usepackage{titling}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{icomma}

\usepackage{csquotes}
\usepackage[backend=biber, style=alphabetic, citestyle=alphabetic, maxcitenames=4, maxbibnames=4, mincitenames=2]{biblatex}

\newcommand{\ginv}{\mathbf f}
\newcommand{\push}{\mathbf g_\star p_\mathbf Z}
\mathcode`\*=\number\cdot
\newcommand{\numberthis}{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\acomm}[1]{\hspace{2.5cm}\text{#1}}
\newcommand{\code}[1]{{\texttt{\small#1}}}

\newcommand{\pro}{\ensuremath{\:\%{}\:}}
\newcommand{\md}{\ensuremath{\text{d}}}
\newcommand{\NN}{\ensuremath{\mathbb N}}
\newcommand{\ZZ}{\ensuremath{\mathbb Z}}
\newcommand{\QQ}{\ensuremath{\mathbb Q}}
\newcommand{\RR}{\ensuremath{\mathbb R}}
\newcommand{\CC}{\ensuremath{\mathbb C}}
\newcommand{\DD}{\ensuremath{\mathbb D}}
\newcommand{\LL}{\ensuremath{\mathbb L}}
\newcommand{\PP}{\ensuremath{\mathbb P}}
\newcommand{\transpose}[1]{\ensuremath{#1^{\textup T}}}

\newcommand{\half}{\ensuremath{\frac{1}{2}}}
\newcommand{\third}{\ensuremath{\frac{1}{3}}}
\newcommand{\fourth}{\ensuremath{\frac{1}{4}}}
\newcommand{\ctp}[1]{\ensuremath{\cdot10^{#1}}}
\newcommand{\reci}{\ensuremath{^{-1}}}

\usepackage[acronym, toc]{glossaries}
\input{acronyms.tex}


\addbibresource{references.bib}

\setlength{\droptitle}{-10ex}

\title{Networks for Variations: A Review of Normalizing Flows for Bayesian Variational Inference}

\author{Søren Winkel Holm}
\date{\today}

\pagestyle{fancy}
\fancyhf{}
\lhead{Søren Winkel Holm}
\chead{}
\rhead{Technical University of Denmark}
\lfoot{The Lottery Ticket Hypothesis}
\rfoot{Page \thepage{} of \pageref{LastPage}}

\graphicspath{{imgs/}}
\linespread{1.15}

\begin{document}
\setlength{\headheight}{15pt}
\addtolength{\topmargin}{-2.5pt}

\maketitle
\thispagestyle{fancy}
% \tableofcontents
 % Explain the basic concepts of normalizing flows and how it related to generative modelling and approximate Bayesian inference
\section*{Introduction}%
\label{sec:Introduction}
In many technical fields, modelling complex systems is in recent years achieved using \acrfull{dl} instead of setting up domain-suitable inferential statistical models \cite{bzdok2018point, breiman2001stat}.
The success of \acrshort{dl} can be attributed to the potential for using similar algorithms to achieve high prediction accuracy across different big data problems \cite{parloff2016dl}.
However, a need for moving these high-accuracy, black box methods towards more robustness and explainability has been highlighted.
Seeking this goal, methods have been developed for characterising complete distributions of model predictions, parametrizations or training data instead of only focusing specific realisations of these.
This distributional view of \acrshort{dl} is used in \acrfull{dgm} and in the wider context \acrfull{bml}.
For both the task of \acrshort{dgm} specifically and the general task of approximating posterior distributions in \acrshort{bml}, robust and general methods for constructing complex distributions are needed.
\acrfull{nf} specify a scalable mechanism allowing for the representation of arbitrary distributions.
This method is here reviewed with a focus on its' relevance for \acrshort{dl}.

\section*{Fundamental Concepts}%
Let 
\begin{equation}
    \mathbf Z \in \RR^2 \sim p_\mathbf Z,
\end{equation}
where $p_\mathbf Z$ is known and analytically tractable distribution, e.g. $p_\mathbf Z=\mathcal N$.
Now using a composition of $N$ bijective functions $\mathbf g=\mathbf g_N \circ \cdots \circ \mathbf g_1$ with inverse $\ginv=\ginv_N \circ \cdots \circ \ginv_1$ and Jacobian $\mathbf D\mathbf g$,
set $\mathbf W=\mathbf g (\mathbf Z)$ giving the density of $\mathbf W$
\begin{equation}\label{eq:dens}
    \push(\mathbf w)= p_\mathbf W(\mathbf w)=\frac{p_\mathbf Z \left(\ginv(\mathbf w)\right)}{\left| \det \mathbf D\mathbf g\left(\ginv(\mathbf w)\right) \right|}.
\end{equation}
In the context of \acrshort{nf}, $\push$ is named the \emph{pushforward} of the base density $p_\mathbf Z$.
$\push$ pushes the simple density $p_\mathbf Z$ to a possibly arbitrarily complex distribution which is called flow in the \emph{generative direction} \cite{koby2021nf} as
\begin{equation}\label{eq:gen}
    \mathbf z \sim p_\mathbf Z \wedge \mathbf w=g(\mathbf z) \Rightarrow \mathbf w \sim \push.
\end{equation}
Inversely, $\ginv$ moves density towards the simple distribution, a process called flow in the \emph{normalizing direction} \cite{koby2021nf}.

Using this construction, arbitrarily complex distributions $p_\mathbf W$ can provenly be represented \cite{boga2007triang}, but the functions are only considered \acrshort{nf} if $\mathbf g_i$, $\ginv_i$ and the Jacobian determinant are easy to compute \cite{koby2021nf} e.g. using 
\begin{align*}
    &\left| \det \mathbf D\mathbf g\left(\ginv(\mathbf w)\right) \right| \reci 
    =\numberthis\\
    &\left| \prod_i^N \det\mathbf D \ginv_i\left(\ginv_{i+1}\circ \cdots \circ \ginv_N(\mathbf w) \right)  \right|.
\end{align*}
$\mathbf g$ may have a parametrization $\phi$, resulting in the pushforward being parameter dependant $\push(\mathbf w|\phi)$.

Using \eqref{eq:dens}, \acrshort{nf} allows for density evaluation and using \eqref{eq:gen} for sampling.
The first quality makes the method relevant for \acrfull{vi} used in \acrshort{bml} for approximating $p=p(\mathbf w | \mathcal D)$ as 
\begin{equation}
    q^\star = \operatorname{argmin}_{q\in \mathcal Q} \mathbb D_{KL}[q || p] 
\end{equation}
where $\mathbb D_{KL}$ is the \acrfull{kl} and $\mathcal Q$ is the variational family of possible approximations \cite{Blei2016VariationalIA}.
Minimization of \acrshort{kl} corresponds to maximization of the \acrfull{elbo} which is, assuming this family is parametrized with $\phi$,
\begin{equation}
    \mathcal L(\phi) = \mathbb E_{q|\phi}[\ln p(\mathbf w, \mathcal D)] - \mathbb E_{q|\phi}[\ln q(\mathbf w|\phi)].
\end{equation}
To optimize this without model-dependant derivations, gradient ascent on the $\mathcal L$ is carried out resulting in \acrfull{bbvi}.
Here, computing gradients of the form $ \nabla_\phi \mathbb E_{q|\phi}[h(\mathbf w)]$ is required.
If \acrshort{nf} are used such that $q(\mathbf w|\phi)=\push(\mathbf w| \phi)$, the gradients can be computed using the reparametrization trick \cite{JimenezRezende2015VariationalIW}
\begin{equation}
    \nabla_\phi \mathbb E_{q|\phi}[h(\mathbf w)]
    =
    \nabla_\phi \mathbb E_{p_\mathbf Z}[h(\mathbf g(\mathbf z | \phi))].
\end{equation}

An alternative use for \acrshort{nf} is directly modelling data as a type of density estimation.
Here, data likelihood is
\begin{align*}
    & \ln p (\mathcal D| \phi)  = \sum_{i=1}^M \ln \push(\mathbf y_i|\phi) = \\
    & \sum_{i=1}^M \left(
        \ln p_\mathbf Z(\ginv(\mathbf y_i|\phi)) + \ln \left| \det \mathbf D\ginv(\mathbf y_i|\phi)\right|,
    \right).
\end{align*}
This model is generative using \eqref{eq:gen} and can be fitted using \acrfull{mle}.

\section*{State of the Art}%
\begin{itemize}
    \item Introduction recently, compare with standard bbvi
    \item Examples of nfs
\end{itemize}

\section*{Open Problems}%
\label{sec:Open Problems}
\begin{itemize}
    \item Choice of base density?
    \item What flows are efficient?
    \item Discrete distributions?
\end{itemize}

\clearpage
\renewcommand*{\bibfont}{\normalfont\footnotesize}
\printbibliography[heading=bibintoc]

\printglossary[type=\acronymtype]
\end{document}
