\documentclass[12pt,fleqn,twocolumn]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}

\usepackage[top=2.5cm, bottom=2.5cm, left=3cm, right=3cm, includeheadfoot]{geometry}

\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{float}
\usepackage{changepage}
\usepackage[nottoc, numbib]{tocbibind}
\usepackage{lastpage}
\usepackage{setspace}
\usepackage[bottom]{footmisc}
\usepackage{titling}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{icomma}

\usepackage{csquotes} % To avoid biber warnings
\usepackage[backend=biber, style=alphabetic, citestyle=alphabetic, maxcitenames=4, maxbibnames=4, mincitenames=2]{biblatex}
\usepackage{dirtytalk}

\mathcode`\*=\number\cdot
\newcommand{\numberthis}{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\acomm}[1]{\hspace{2.5cm}\text{#1}}
\newcommand{\code}[1]{{\texttt{\small#1}}}

\newcommand{\md}{\ensuremath{\text{d}}}
\newcommand{\NN}{\ensuremath{\mathbb N}}
\newcommand{\ZZ}{\ensuremath{\mathbb Z}}
\newcommand{\QQ}{\ensuremath{\mathbb Q}}
\newcommand{\RR}{\ensuremath{\mathbb R}}
\newcommand{\CC}{\ensuremath{\mathbb C}}
\newcommand{\LL}{\ensuremath{\mathbb L}}
\newcommand{\PP}{\ensuremath{\mathbb P}}
\newcommand{\transpose}[1]{\ensuremath{#1^{\textup T}}}

\newcommand{\half}{\ensuremath{\frac{1}{2}}}
\newcommand{\third}{\ensuremath{\frac{1}{3}}}
\newcommand{\fourth}{\ensuremath{\frac{1}{4}}}
\newcommand{\ctp}[1]{\ensuremath{\cdot10^{#1}}}
\newcommand{\reci}{\ensuremath{^{-1}}}

\usepackage[acronym, toc]{glossaries}
\input{acronyms.tex}


\addbibresource{references.bib}

\setlength{\droptitle}{-10ex}

\title{Hush-hush Gradients: A Review of Differential Privacy for Deep Learning}

\author{Søren Winkel Holm}
\date{\today}

\pagestyle{fancy}
\fancyhf{}
\lhead{Søren Winkel Holm}
\chead{}
\rhead{Technical University of Denmark}
\lfoot{Differential Privacy for Deep Learning}
\rfoot{Page \thepage{} of \pageref{LastPage}}

\graphicspath{{imgs/}}
\linespread{1.15}

\begin{document}
\setlength{\headheight}{15pt}
\addtolength{\topmargin}{-2.5pt}

\maketitle
\thispagestyle{fancy}
% \tableofcontents

\section*{Introduction}%
\label{sec:Introduction}
The field of \acrfull{dl} is for many subfields moving towards a setup where large multi-purpose foundation models are developed and trained at major companies or research instutitions, and then released for engineers to adapt to specific applications \cite[pp. 3]{bommasani2021oppurt}.
This application of the open-source principle to pre-trained models improves scientific reproduction ability \cite[pp. 3]{hartley2020dtool} and technology accessibility \cite[pp. 139]{bommasani2021oppurt}.
One risk, however, is an adversarial actor exploiting a property of \acrshort{dl} models: 
Parts of training data is generally recoverable from model weights \cite{nasr2019privacy, shokri2017membership}.
This might expose proprietary data or the private data of individuals as exemplified for \acrfull{nlp} language models in Figure \ref{fig:extracting.pdf}.
As large-scale data sets are here to stay \cite{sun2017unreasonable}, algorithmic methods for improving the privacy of foundation models are required.
The methods of \acrfull{dp} are suitable for this task and the relevant concepts, algorithms and problems will here be reviewed.

\begin{figure}[H]
    \centering
        \includegraphics[clip, trim=11.5cm 12cm 2.5cm 8cm, width=.8\linewidth]{extracting.pdf}
        \caption{The extraction attack performed on GPT-2 \cite[Fig. 1]{carlini2021extracting} (private data redacted).}
    \label{fig:extracting.pdf}
\end{figure}\noindent

\section*{Fundamental Concepts}%
\label{sec:Fundamental Concepts}
Achieving \acrshort{dp} corresponds to making a promise of hiding information about individuals when publishing quantitative patterns about groups \cite[pp. 5]{dwork2014alg}.
This general problem is historically faced in releases of statistical data analyses by e.g. official organizations \cite{dalenius1977stat, wiki2022diff}.
An algorithm is thus differentially private if a third party observer cannot extract individual information from its' computation.
% The goal can has been robustly defined \cite[pp. 5]{dwork2014alg} as
% \begin{displayquote}
% Differential privacy describes a promise, made by a data holder, or curator, to a data subject: “You will not be affected, adversely or otherwise, by allowing your data to be used in any study or analysis, no matter what other studies, data sets, or information sources, are available.”
% \end{displayquote}
In this context, a \acrfull{ml} model \(f(x|w)=\hat y \approx y\) trained on a data set \(\mathcal D\) exposes data patterns when either its' parametrization $w$ or predictions $(x, \hat y)$ are released.

\section*{State of the Art}%
\label{sec:State of The Art}

\section*{Open Problems}%
\label{sec:Open Problems}

\clearpage
\renewcommand*{\bibfont}{\normalfont\footnotesize}
\printbibliography[heading=bibintoc]

\printglossary[type=\acronymtype]

\end{document}
