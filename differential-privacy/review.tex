\documentclass[12pt,fleqn,twocolumn]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}

\usepackage[top=2.5cm, bottom=2.5cm, left=3cm, right=3cm, includeheadfoot]{geometry}

\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{float}
\usepackage{changepage}
\usepackage[nottoc, numbib]{tocbibind}
\usepackage{lastpage}
\usepackage{setspace}
\usepackage[bottom]{footmisc}
\usepackage{titling}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{icomma}

\usepackage{csquotes}
\usepackage[backend=biber, style=alphabetic, citestyle=alphabetic, maxcitenames=4, maxbibnames=4, mincitenames=2]{biblatex}

\mathcode`\*=\number\cdot
\newcommand{\numberthis}{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\acomm}[1]{\hspace{2.5cm}\text{#1}}
\newcommand{\code}[1]{{\texttt{\small#1}}}

\newcommand{\pro}{\ensuremath{\:\%{}\:}}
\newcommand{\md}{\ensuremath{\text{d}}}
\newcommand{\NN}{\ensuremath{\mathbb N}}
\newcommand{\ZZ}{\ensuremath{\mathbb Z}}
\newcommand{\QQ}{\ensuremath{\mathbb Q}}
\newcommand{\RR}{\ensuremath{\mathbb R}}
\newcommand{\CC}{\ensuremath{\mathbb C}}
\newcommand{\DD}{\ensuremath{\mathbb D}}
\newcommand{\LL}{\ensuremath{\mathbb L}}
\newcommand{\PP}{\ensuremath{\mathbb P}}
\newcommand{\transpose}[1]{\ensuremath{#1^{\textup T}}}

\newcommand{\half}{\ensuremath{\frac{1}{2}}}
\newcommand{\third}{\ensuremath{\frac{1}{3}}}
\newcommand{\fourth}{\ensuremath{\frac{1}{4}}}
\newcommand{\ctp}[1]{\ensuremath{\cdot10^{#1}}}
\newcommand{\reci}{\ensuremath{^{-1}}}

\usepackage[acronym, toc]{glossaries}
\input{acronyms.tex}


\addbibresource{references.bib}

\setlength{\droptitle}{-10ex}

\title{Hush-hush Gradients: A Review of Differential Privacy for Deep Learning}

\author{Søren Winkel Holm}
\date{\today}

\pagestyle{fancy}
\fancyhf{}
\lhead{Søren Winkel Holm}
\chead{}
\rhead{Technical University of Denmark}
\lfoot{Differential Privacy for Deep Learning}
\rfoot{Page \thepage{} of \pageref{LastPage}}

\graphicspath{{imgs/}}
\linespread{1.15}

\begin{document}
\setlength{\headheight}{15pt}
\addtolength{\topmargin}{-2.5pt}

\maketitle
\thispagestyle{fancy}
% \tableofcontents

\section*{Introduction}%
\label{sec:Introduction}
The field of \acrfull{dl} is for many subfields moving towards a setup where large multi-purpose foundation models are developed and trained at major companies or research instutitions, and then released for engineers to adapt to specific applications \cite[pp. 3]{bommasani2021oppurt}.
This application of the open-source principle to pre-trained models improves scientific reproduction ability \cite[pp. 3]{hartley2020dtool} and technology accessibility \cite[pp. 139]{bommasani2021oppurt}.
One risk, however, is an adversarial actor exploiting a property of \acrshort{dl} models: 
Parts of training data is generally recoverable from model weights \cite{nasr2019privacy, shokri2017membership}.
This might expose proprietary data or the private data of individuals as exemplified for \acrfull{nlp} language models in Figure \ref{fig:extracting.pdf}.
As large-scale data sets are here to stay \cite{sun2017unreasonable}, algorithmic methods for improving the privacy of foundation models are required.
The methods of \acrfull{dp} are suitable for this task and the relevant concepts, algorithms and problems will here be reviewed.

\begin{figure}[H]
    \centering
        \includegraphics[clip, trim=11.5cm 12cm 2.5cm 8cm, width=.8\linewidth]{extracting.pdf}
        \caption{The private data exposed by GPT-2 found using a simple extraction attack \cite[Fig. 1]{carlini2021extracting} (private data redacted).}
    \label{fig:extracting.pdf}
\end{figure}\noindent

\section*{Fundamental Concepts}%
\label{sec:Fundamental Concepts}
Achieving \acrshort{dp} corresponds to making a promise of hiding information about individuals when publishing quantitative patterns about groups \cite[pp. 5]{dwork2014alg}.
This general problem is historically faced in releases of statistical data analyses by e.g. official organizations \cite{dalenius1977stat, wiki2022diff}.
An algorithm is thus differentially private if a third party observer cannot extract individual information from its' computation.
In this context, a \acrfull{ml} model \(f(x|w)=\hat y \approx y\) trained on a data set \(\mathcal D\) exposes data patterns when either its' parametrization $w$ or predictions $(x, \hat y)$ are released.

Let $\mathcal D_i \in \DD$ be a data set containing the private information of individual $i$ and $\mathcal D_{\hat i} \in \DD$ be identical except excluding this private information.
For most approaches, the goal is to maximise \acrshort{dp} by minimising the impact of individual data on computation which can be be quantified by measures such as $\ell_1$-sensitivity \cite[pp.31]{dwork2014alg} $\Delta g$ of a numeric statistic $g: \DD \rightarrow \RR^k$,
\begin{equation}
    \Delta g= \operatorname{max}_{i, \hat i \in \DD} ||g(x)-g(y)||_1.
\end{equation}
To achieve this, additive noise mechanisms can be used.
For $\Delta g$, this can be achieved by adding Gaussian noise to outputs \cite[D 3.3]{dwork2014alg} 
\begin{equation}\label{eq:gauss}
  f(x)+(Y_1, \ldots, Y_k), Y_i \sim \mathcal N(0, \sigma_{\Delta g}^2)  
\end{equation}
The end goal of such \acrshort{dp} mechanisms on random algorithms $\mathcal F(\mathcal D)$ outputting $w \in \operatorname{Im}(\mathcal F)$ with probability $p_{\mathcal F}(w|\mathcal D)$, is to guarantee $(\varepsilon, \delta)$-privacy \cite[Def. 2.4]{dwork2014alg} requiring $\forall S \in \operatorname{Im}(\mathcal F)$ that
\begin{equation}
    P\left(
        \mathcal{F}(\mathcal D_i) \in S
    \right)
    \leq
    \exp(\varepsilon)
    P\left(
        \mathcal{F}(\mathcal D_{\hat i}) \in S
    \right)
    +\delta.
\end{equation}
Thus, for $1-\delta$ of the probability density over algorithmic randomness, it is promised that adding your private data to $\mathcal D$ does not raise your risk of harm by more than $\exp(\varepsilon)$ \cite[pp. 21]{dwork2014alg}.
Often, $\delta=0$, requring the stronger $\varepsilon$-privacy \cite{wiki2022diff}.
For the Gaussian additive noise mechanism \eqref{eq:gauss}, $(\varepsilon, \delta)$-privacy is achieved when $\sigma_{\Delta g}^2=\Delta g \ln(\nicefrac 1 \delta)\varepsilon\reci$ \cite[App. A]{dwork2014alg}.

\acrshort{ml} training procedures are randomized algorithms and as such, simple $(\varepsilon, \delta)$-privacy can be applied directly, though many approaches such as additive noise mechanisms raise the number of samples required to obtain similar performance \cite[pp.221]{dwork2014alg}.
A practical way to integrate the \acrshort{dp} mechanism into \acrshort{dl} training is to modify how the loss gradient estimate $g_t = \nabla \mathcal L(\hat y, y | w_t)$ is used by the optimizer $w_{t+1} = w_t - \eta_t m(g_t)$ \cite{rade2019tensorflow}.
$m$ could add noise or clip the gradient \cite{abadi2016dldp}.

\section*{State of the Art}%
\label{sec:State of The Art}
The foundational application of \acrshort{dp} to \acrshort{dl} was performed at Google by \textcite{abadi2016dldp} in \citeyear{abadi2016dldp} where \acrfull{dpsgd} was presented.
This algorithm modifies the gradient estimate of a $B$-sized batch by setting
\begin{align*}
    m(g_t) &= \frac 1 B \Big(
        \sum_{i=0}^B \frac{\mathbf g_t(x_i)}{\operatorname{max}\left(1, || \mathbf g_t(x_i)||_2 C\reci\right)}
    \\ &+ \mathbf e \Big), \mathbf e\sim \mathcal N(0, \sigma^2C^2 \mathbf I),
\end{align*}
where the gradient clipping hyperparameter $C$ and the noise hyperparameter $\sigma$ can be chosen such that is $(\varepsilon, \delta)$-privacy can be guaranteed for any $\varepsilon, \delta > 0$ \cite[Ch. 3.1]{abadi2016dldp}.
When requiring $(8, 10\ctp{-5})$-privacy, the paper finds performance drops of $1.3\pro$ for MNIST and $7\pro$ for CIFAR-10 \cite[Chap. 5.3]{abadi2016dldp} and that computational time was increased by the requirement of single example gradients $\mathbf g_t(x_i)$ especially for convolutional layers \cite[Chap. 4]{abadi2016dldp}.
The clipping performed in $m$ is used for assuring an upper bound on sensitivity from which the correct noise in the Gaussian additive noise mechanism can be used \cite[Chap. 4]{abadi2016dldp}.

\acrshort{dpsgd} remains highly influential and is used as default in leading implementations TensorFlow Privacy \cite{rade2019tensorflow} and PyTorch Opacus \cite{yousef2021opacus}.

\acrshort{dpsgd} was swiftly combined with the another main \acrshort{dl} privacy tool, \acrfull{fl} by \textcite{mcmahan2017learningdp} in \citeyear{mcmahan2017learningdp}, producing \acrfull{dpfa} which was used for training a high-performance language model with privacy guarantees \cite[Chap. 3]{mcmahan2017learningdp}.


% https://www.semanticscholar.org/paper/Adaptive-Laplace-Mechanism%3A-Differential-Privacy-in-Phan-Wu/1f8be49d63c694ec71c2310309cd02a2d8dd457f
% https://www.semanticscholar.org/paper/A-General-Approach-to-Adding-Differential-Privacy-McMahan-Andrew/f4966c81b9fe6476e8d418b75308b36f44865541
% https://www.semanticscholar.org/paper/Differentially-Private-Model-Publishing-for-Deep-Yu-Liu/795f68480f51d4359b85b04d1c7edd41423bf4c2

\section*{Open Problems}%
\label{sec:Open Problems}

\clearpage
\renewcommand*{\bibfont}{\normalfont\footnotesize}
\printbibliography[heading=bibintoc]

\printglossary[type=\acronymtype]

\end{document}
